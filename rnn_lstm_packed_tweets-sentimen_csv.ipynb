{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting data\n",
    "\n",
    "    - Sentiment140 dataset: <http://help.sentiment140.com/for-students>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetsDF = pd.read_csv(\"training.1600000.processed.noemoticon.csv\", engine=\"python\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0           1                             2         3                4  \\\n",
       "0  0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY  _TheSpecialOne_   \n",
       "1  0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY    scotthamilton   \n",
       "2  0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY         mattycus   \n",
       "3  0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY          ElleCTF   \n",
       "4  0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY           Karoli   \n",
       "\n",
       "                                                   5  \n",
       "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1  is upset that he can't update his Facebook by ...  \n",
       "2  @Kenichan I dived many times for the ball. Man...  \n",
       "3    my whole body feels itchy and like its on fire   \n",
       "4  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweetsDF.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetsDF['sentiment_cat'] = tweetsDF[0].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetsDF[\"sentiment\"] = tweetsDF['sentiment_cat'].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetsDF.to_csv('train_processed.csv', header=None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetsDF.sample(10000).to_csv('train_processed_sample.csv', header=None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0           1                             2         3                4  \\\n",
       "0  0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY  _TheSpecialOne_   \n",
       "1  0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY    scotthamilton   \n",
       "2  0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY         mattycus   \n",
       "3  0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY          ElleCTF   \n",
       "4  0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY           Karoli   \n",
       "\n",
       "                                                   5  6  7  \n",
       "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...  0  0  \n",
       "1  is upset that he can't update his Facebook by ...  0  0  \n",
       "2  @Kenichan I dived many times for the ball. Man...  0  0  \n",
       "3    my whole body feels itchy and like its on fire   0  0  \n",
       "4  @nationwideclass no, it's not behaving at all....  0  0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "tweets = pd.read_csv('train_processed.csv', engine='python', header=None)\n",
    "tweets.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>5</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   5  7\n",
       "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...  0\n",
       "1  is upset that he can't update his Facebook by ...  0\n",
       "2  @Kenichan I dived many times for the ball. Man...  0\n",
       "3    my whole body feels itchy and like its on fire   0\n",
       "4  @nationwideclass no, it's not behaving at all....  0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_tweets = tweets.iloc[:, [5,7]]\n",
    "label_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_tweets.to_csv('train_labeled.csv', header=None, index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining fileds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL = data.LabelField(dtype=torch.float)\n",
    "TWEET = data.Field(sequential=True, tokenize='spacy', include_lengths=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [('tweet', TWEET), ('label', LABEL)]\n",
    "\n",
    "twitterDataset = data.TabularDataset(path='train_labeled.csv', format='CSV', skip_header=False, fields=fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "(train, valid, test) = twitterDataset.split(split_ratio=[0.8, 0.1, 0.1], random_state=random.seed(123))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num train: 1280000, valid: 160000, test: 160000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Num train: {len(train)}, valid: {len(valid)}, test: {len(test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Builing a vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 20000\n",
    "TWEET.build_vocab(train, max_size=vocab_size)\n",
    "LABEL.build_vocab(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('!', 724072),\n",
       " ('.', 647596),\n",
       " ('I', 527196),\n",
       " (' ', 469727),\n",
       " ('to', 447002),\n",
       " ('the', 392317),\n",
       " (',', 386838),\n",
       " ('a', 295794),\n",
       " ('i', 271292),\n",
       " ('my', 226121)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TWEET.vocab.freqs.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'0': 639980, '1': 640020})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LABEL.vocab.freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device('cpu')\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits((train, valid, test), batch_size=32, sort_within_batch = True, sort_key=lambda x: len(x.tweet), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "Text matrix size: torch.Size([19, 32])\n",
      "Target vector size: torch.Size([32])\n",
      "\n",
      "Valid\n",
      "Text matrix size: torch.Size([1, 32])\n",
      "Target vector size: torch.Size([32])\n",
      "\n",
      "Test\n",
      "Text matrix size: torch.Size([1, 32])\n",
      "Target vector size: torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "print('Train')\n",
    "for batch in train_iterator:\n",
    "    print(f\"Text matrix size: {batch.tweet[0].size()}\")\n",
    "    print(f\"Target vector size: {batch.label.size()}\")\n",
    "    break\n",
    "\n",
    "print(\"\\nValid\")\n",
    "for batch in valid_iterator:\n",
    "    print(f\"Text matrix size: {batch.tweet[0].size()}\")\n",
    "    print(f\"Target vector size: {batch.label.size()}\")\n",
    "    break\n",
    "\n",
    "print(\"\\nTest\")\n",
    "for batch in test_iterator:\n",
    "    print(f\"Text matrix size: {batch.tweet[0].size()}\")\n",
    "    print(f\"Target vector size: {batch.label.size()}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creat a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class TwitterLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super(TwitterLSTM, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.encoder = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim)\n",
    "        self.predictor = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, text, text_length):\n",
    "        embedded = self.embedding(text)\n",
    "        \n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, text_length)\n",
    "        \n",
    "        packed_output, (hidden, cell) = self.encoder(packed)\n",
    "        preds = self.predictor(hidden.squeeze(0))\n",
    "        \n",
    "        return preds.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TwitterLSTM(\n",
       "  (embedding): Embedding(20002, 128)\n",
       "  (encoder): LSTM(128, 256)\n",
       "  (predictor): Linear(in_features=256, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TwitterLSTM(vocab_size=20002, embedding_dim=128, hidden_dim=256, output_dim=1)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-2)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_binary_accuracy(model, data_loader, device):\n",
    "    model.eval()\n",
    "    correct_pred, num_examples = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch_data in enumerate(data_loader):\n",
    "            text, text_lengths = batch_data.tweet\n",
    "            logits = model(text, text_lengths)\n",
    "            predicted_labels = (torch.sigmoid(logits) > 0.5).long()\n",
    "            num_examples += batch_data.label.size(0)\n",
    "            correct_pred += (predicted_labels.long() == batch_data.label.long()).sum()\n",
    "        return correct_pred.float() / num_examples * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch.nn.functional as F\n",
    "\n",
    "NUM_EPOCHS = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/007 | Batch 000/40000 | Cost: 0.6494\n",
      "Epoch: 001/007 | Batch 1000/40000 | Cost: 0.6475\n",
      "Epoch: 001/007 | Batch 2000/40000 | Cost: 0.8558\n",
      "Epoch: 001/007 | Batch 3000/40000 | Cost: 0.4507\n",
      "Epoch: 001/007 | Batch 4000/40000 | Cost: 0.7720\n",
      "Epoch: 001/007 | Batch 5000/40000 | Cost: 0.7869\n",
      "Epoch: 001/007 | Batch 6000/40000 | Cost: 0.8636\n",
      "Epoch: 001/007 | Batch 7000/40000 | Cost: 0.6355\n",
      "Epoch: 001/007 | Batch 8000/40000 | Cost: 0.7200\n",
      "Epoch: 001/007 | Batch 9000/40000 | Cost: 0.7128\n",
      "Epoch: 001/007 | Batch 10000/40000 | Cost: 0.6936\n",
      "Epoch: 001/007 | Batch 11000/40000 | Cost: 0.8777\n",
      "Epoch: 001/007 | Batch 12000/40000 | Cost: 0.7586\n",
      "Epoch: 001/007 | Batch 13000/40000 | Cost: 0.6140\n",
      "Epoch: 001/007 | Batch 14000/40000 | Cost: 0.7572\n",
      "Epoch: 001/007 | Batch 15000/40000 | Cost: 0.5550\n",
      "Epoch: 001/007 | Batch 16000/40000 | Cost: 0.6876\n",
      "Epoch: 001/007 | Batch 17000/40000 | Cost: 0.6643\n",
      "Epoch: 001/007 | Batch 18000/40000 | Cost: 0.7760\n",
      "Epoch: 001/007 | Batch 19000/40000 | Cost: 0.6778\n",
      "Epoch: 001/007 | Batch 20000/40000 | Cost: 0.7279\n",
      "Epoch: 001/007 | Batch 21000/40000 | Cost: 0.5418\n",
      "Epoch: 001/007 | Batch 22000/40000 | Cost: 0.7599\n",
      "Epoch: 001/007 | Batch 23000/40000 | Cost: 0.6810\n",
      "Epoch: 001/007 | Batch 24000/40000 | Cost: 0.5559\n",
      "Epoch: 001/007 | Batch 25000/40000 | Cost: 0.6277\n",
      "Epoch: 001/007 | Batch 26000/40000 | Cost: 0.6128\n",
      "Epoch: 001/007 | Batch 27000/40000 | Cost: 0.7002\n",
      "Epoch: 001/007 | Batch 28000/40000 | Cost: 0.9042\n",
      "Epoch: 001/007 | Batch 29000/40000 | Cost: 0.5942\n",
      "Epoch: 001/007 | Batch 30000/40000 | Cost: 0.5994\n",
      "Epoch: 001/007 | Batch 31000/40000 | Cost: 0.7132\n",
      "Epoch: 001/007 | Batch 32000/40000 | Cost: 0.6517\n",
      "Epoch: 001/007 | Batch 33000/40000 | Cost: 1.0372\n",
      "Epoch: 001/007 | Batch 34000/40000 | Cost: 0.7088\n",
      "Epoch: 001/007 | Batch 35000/40000 | Cost: 0.8425\n",
      "Epoch: 001/007 | Batch 36000/40000 | Cost: 0.6635\n",
      "Epoch: 001/007 | Batch 37000/40000 | Cost: 0.7645\n",
      "Epoch: 001/007 | Batch 38000/40000 | Cost: 0.7807\n",
      "Epoch: 001/007 | Batch 39000/40000 | Cost: 0.5784\n",
      "training accuracy: 63.50%\n",
      "valid accuracy: 63.27%\n",
      "Time elapsed: 66.09 min\n",
      "Epoch: 002/007 | Batch 000/40000 | Cost: 0.7263\n",
      "Epoch: 002/007 | Batch 1000/40000 | Cost: 0.5230\n",
      "Epoch: 002/007 | Batch 2000/40000 | Cost: 0.6390\n",
      "Epoch: 002/007 | Batch 3000/40000 | Cost: 0.5465\n",
      "Epoch: 002/007 | Batch 4000/40000 | Cost: 0.6025\n",
      "Epoch: 002/007 | Batch 5000/40000 | Cost: 0.5357\n",
      "Epoch: 002/007 | Batch 6000/40000 | Cost: 0.6863\n",
      "Epoch: 002/007 | Batch 7000/40000 | Cost: 0.7540\n",
      "Epoch: 002/007 | Batch 8000/40000 | Cost: 0.8871\n",
      "Epoch: 002/007 | Batch 9000/40000 | Cost: 0.6146\n",
      "Epoch: 002/007 | Batch 10000/40000 | Cost: 0.5771\n",
      "Epoch: 002/007 | Batch 11000/40000 | Cost: 0.7030\n",
      "Epoch: 002/007 | Batch 12000/40000 | Cost: 0.5723\n",
      "Epoch: 002/007 | Batch 13000/40000 | Cost: 0.6464\n",
      "Epoch: 002/007 | Batch 14000/40000 | Cost: 0.8074\n",
      "Epoch: 002/007 | Batch 15000/40000 | Cost: 0.5306\n",
      "Epoch: 002/007 | Batch 16000/40000 | Cost: 0.6748\n",
      "Epoch: 002/007 | Batch 17000/40000 | Cost: 0.6805\n",
      "Epoch: 002/007 | Batch 18000/40000 | Cost: 0.8434\n",
      "Epoch: 002/007 | Batch 19000/40000 | Cost: 0.6681\n",
      "Epoch: 002/007 | Batch 20000/40000 | Cost: 0.6169\n",
      "Epoch: 002/007 | Batch 21000/40000 | Cost: 0.5268\n",
      "Epoch: 002/007 | Batch 22000/40000 | Cost: 0.6231\n",
      "Epoch: 002/007 | Batch 23000/40000 | Cost: 0.6999\n",
      "Epoch: 002/007 | Batch 24000/40000 | Cost: 0.7162\n",
      "Epoch: 002/007 | Batch 25000/40000 | Cost: 0.4380\n",
      "Epoch: 002/007 | Batch 26000/40000 | Cost: 0.4639\n",
      "Epoch: 002/007 | Batch 27000/40000 | Cost: 0.7083\n",
      "Epoch: 002/007 | Batch 28000/40000 | Cost: 0.4653\n",
      "Epoch: 002/007 | Batch 29000/40000 | Cost: 0.6846\n",
      "Epoch: 002/007 | Batch 30000/40000 | Cost: 0.5806\n",
      "Epoch: 002/007 | Batch 31000/40000 | Cost: 0.6701\n",
      "Epoch: 002/007 | Batch 32000/40000 | Cost: 0.7700\n",
      "Epoch: 002/007 | Batch 33000/40000 | Cost: 0.7658\n",
      "Epoch: 002/007 | Batch 34000/40000 | Cost: 0.6964\n",
      "Epoch: 002/007 | Batch 35000/40000 | Cost: 0.5158\n",
      "Epoch: 002/007 | Batch 36000/40000 | Cost: 0.6051\n",
      "Epoch: 002/007 | Batch 37000/40000 | Cost: 0.5840\n",
      "Epoch: 002/007 | Batch 38000/40000 | Cost: 0.7001\n",
      "Epoch: 002/007 | Batch 39000/40000 | Cost: 0.7304\n",
      "training accuracy: 65.31%\n",
      "valid accuracy: 64.98%\n",
      "Time elapsed: 127.90 min\n",
      "Epoch: 003/007 | Batch 000/40000 | Cost: 0.6722\n",
      "Epoch: 003/007 | Batch 1000/40000 | Cost: 0.6818\n",
      "Epoch: 003/007 | Batch 2000/40000 | Cost: 0.9064\n",
      "Epoch: 003/007 | Batch 3000/40000 | Cost: 0.7183\n",
      "Epoch: 003/007 | Batch 4000/40000 | Cost: 0.5637\n",
      "Epoch: 003/007 | Batch 5000/40000 | Cost: 0.5768\n",
      "Epoch: 003/007 | Batch 6000/40000 | Cost: 0.6495\n",
      "Epoch: 003/007 | Batch 7000/40000 | Cost: 0.6812\n",
      "Epoch: 003/007 | Batch 8000/40000 | Cost: 0.9168\n",
      "Epoch: 003/007 | Batch 9000/40000 | Cost: 0.6721\n",
      "Epoch: 003/007 | Batch 10000/40000 | Cost: 0.6452\n",
      "Epoch: 003/007 | Batch 11000/40000 | Cost: 0.5053\n",
      "Epoch: 003/007 | Batch 12000/40000 | Cost: 0.4269\n",
      "Epoch: 003/007 | Batch 13000/40000 | Cost: 0.5746\n",
      "Epoch: 003/007 | Batch 14000/40000 | Cost: 0.7291\n",
      "Epoch: 003/007 | Batch 15000/40000 | Cost: 0.5601\n",
      "Epoch: 003/007 | Batch 16000/40000 | Cost: 0.6620\n",
      "Epoch: 003/007 | Batch 17000/40000 | Cost: 0.6373\n",
      "Epoch: 003/007 | Batch 18000/40000 | Cost: 0.8367\n",
      "Epoch: 003/007 | Batch 19000/40000 | Cost: 0.6152\n",
      "Epoch: 003/007 | Batch 20000/40000 | Cost: 0.7380\n",
      "Epoch: 003/007 | Batch 21000/40000 | Cost: 0.7896\n",
      "Epoch: 003/007 | Batch 22000/40000 | Cost: 0.6188\n",
      "Epoch: 003/007 | Batch 23000/40000 | Cost: 0.7022\n",
      "Epoch: 003/007 | Batch 24000/40000 | Cost: 0.7116\n",
      "Epoch: 003/007 | Batch 25000/40000 | Cost: 0.6025\n",
      "Epoch: 003/007 | Batch 26000/40000 | Cost: 0.4899\n",
      "Epoch: 003/007 | Batch 27000/40000 | Cost: 0.6170\n",
      "Epoch: 003/007 | Batch 28000/40000 | Cost: 0.7169\n",
      "Epoch: 003/007 | Batch 29000/40000 | Cost: 0.6793\n",
      "Epoch: 003/007 | Batch 30000/40000 | Cost: 0.6052\n",
      "Epoch: 003/007 | Batch 31000/40000 | Cost: 0.5361\n",
      "Epoch: 003/007 | Batch 32000/40000 | Cost: 0.6361\n",
      "Epoch: 003/007 | Batch 33000/40000 | Cost: 0.5379\n",
      "Epoch: 003/007 | Batch 34000/40000 | Cost: 0.6385\n",
      "Epoch: 003/007 | Batch 35000/40000 | Cost: 0.4971\n",
      "Epoch: 003/007 | Batch 36000/40000 | Cost: 0.7825\n",
      "Epoch: 003/007 | Batch 37000/40000 | Cost: 0.6985\n",
      "Epoch: 003/007 | Batch 38000/40000 | Cost: 0.6054\n",
      "Epoch: 003/007 | Batch 39000/40000 | Cost: 0.7048\n",
      "training accuracy: 64.82%\n",
      "valid accuracy: 64.32%\n",
      "Time elapsed: 190.73 min\n",
      "Epoch: 004/007 | Batch 000/40000 | Cost: 0.6691\n",
      "Epoch: 004/007 | Batch 1000/40000 | Cost: 0.5577\n",
      "Epoch: 004/007 | Batch 2000/40000 | Cost: 0.6518\n",
      "Epoch: 004/007 | Batch 3000/40000 | Cost: 0.5435\n",
      "Epoch: 004/007 | Batch 4000/40000 | Cost: 0.5637\n",
      "Epoch: 004/007 | Batch 5000/40000 | Cost: 0.6391\n",
      "Epoch: 004/007 | Batch 6000/40000 | Cost: 0.6652\n",
      "Epoch: 004/007 | Batch 7000/40000 | Cost: 0.7752\n",
      "Epoch: 004/007 | Batch 8000/40000 | Cost: 0.8982\n",
      "Epoch: 004/007 | Batch 9000/40000 | Cost: 0.5484\n",
      "Epoch: 004/007 | Batch 10000/40000 | Cost: 0.5293\n",
      "Epoch: 004/007 | Batch 11000/40000 | Cost: 0.5271\n",
      "Epoch: 004/007 | Batch 12000/40000 | Cost: 0.7308\n",
      "Epoch: 004/007 | Batch 13000/40000 | Cost: 0.5870\n",
      "Epoch: 004/007 | Batch 14000/40000 | Cost: 0.5981\n",
      "Epoch: 004/007 | Batch 15000/40000 | Cost: 0.4820\n",
      "Epoch: 004/007 | Batch 16000/40000 | Cost: 0.6128\n",
      "Epoch: 004/007 | Batch 17000/40000 | Cost: 0.6880\n",
      "Epoch: 004/007 | Batch 18000/40000 | Cost: 0.5353\n",
      "Epoch: 004/007 | Batch 19000/40000 | Cost: 0.6328\n",
      "Epoch: 004/007 | Batch 20000/40000 | Cost: 0.7078\n",
      "Epoch: 004/007 | Batch 21000/40000 | Cost: 0.6540\n",
      "Epoch: 004/007 | Batch 22000/40000 | Cost: 0.6416\n",
      "Epoch: 004/007 | Batch 23000/40000 | Cost: 0.6050\n",
      "Epoch: 004/007 | Batch 24000/40000 | Cost: 0.6804\n",
      "Epoch: 004/007 | Batch 25000/40000 | Cost: 0.5641\n",
      "Epoch: 004/007 | Batch 26000/40000 | Cost: 0.6733\n",
      "Epoch: 004/007 | Batch 27000/40000 | Cost: 0.6280\n",
      "Epoch: 004/007 | Batch 28000/40000 | Cost: 0.6704\n",
      "Epoch: 004/007 | Batch 29000/40000 | Cost: 0.5565\n",
      "Epoch: 004/007 | Batch 30000/40000 | Cost: 0.5701\n",
      "Epoch: 004/007 | Batch 31000/40000 | Cost: 0.8844\n",
      "Epoch: 004/007 | Batch 32000/40000 | Cost: 0.5930\n",
      "Epoch: 004/007 | Batch 33000/40000 | Cost: 0.4868\n",
      "Epoch: 004/007 | Batch 34000/40000 | Cost: 0.4488\n",
      "Epoch: 004/007 | Batch 35000/40000 | Cost: 0.6675\n",
      "Epoch: 004/007 | Batch 36000/40000 | Cost: 0.8427\n",
      "Epoch: 004/007 | Batch 37000/40000 | Cost: 0.8052\n",
      "Epoch: 004/007 | Batch 38000/40000 | Cost: 0.6671\n",
      "Epoch: 004/007 | Batch 39000/40000 | Cost: 0.6874\n",
      "training accuracy: 64.12%\n",
      "valid accuracy: 63.61%\n",
      "Time elapsed: 253.00 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 005/007 | Batch 000/40000 | Cost: 0.5910\n",
      "Epoch: 005/007 | Batch 1000/40000 | Cost: 0.6108\n",
      "Epoch: 005/007 | Batch 2000/40000 | Cost: 0.6353\n",
      "Epoch: 005/007 | Batch 3000/40000 | Cost: 0.6381\n",
      "Epoch: 005/007 | Batch 4000/40000 | Cost: 0.6868\n",
      "Epoch: 005/007 | Batch 5000/40000 | Cost: 0.6500\n",
      "Epoch: 005/007 | Batch 6000/40000 | Cost: 0.5197\n",
      "Epoch: 005/007 | Batch 7000/40000 | Cost: 0.7053\n",
      "Epoch: 005/007 | Batch 8000/40000 | Cost: 0.6093\n",
      "Epoch: 005/007 | Batch 9000/40000 | Cost: 0.7537\n",
      "Epoch: 005/007 | Batch 10000/40000 | Cost: 0.6125\n",
      "Epoch: 005/007 | Batch 11000/40000 | Cost: 0.7172\n",
      "Epoch: 005/007 | Batch 12000/40000 | Cost: 0.7553\n",
      "Epoch: 005/007 | Batch 13000/40000 | Cost: 0.6241\n",
      "Epoch: 005/007 | Batch 14000/40000 | Cost: 0.7913\n",
      "Epoch: 005/007 | Batch 15000/40000 | Cost: 0.7388\n",
      "Epoch: 005/007 | Batch 16000/40000 | Cost: 0.8298\n",
      "Epoch: 005/007 | Batch 17000/40000 | Cost: 0.6015\n",
      "Epoch: 005/007 | Batch 18000/40000 | Cost: 0.6838\n",
      "Epoch: 005/007 | Batch 19000/40000 | Cost: 0.6474\n",
      "Epoch: 005/007 | Batch 20000/40000 | Cost: 0.6219\n",
      "Epoch: 005/007 | Batch 21000/40000 | Cost: 0.5661\n",
      "Epoch: 005/007 | Batch 22000/40000 | Cost: 0.7048\n",
      "Epoch: 005/007 | Batch 23000/40000 | Cost: 0.6535\n",
      "Epoch: 005/007 | Batch 24000/40000 | Cost: 0.5584\n",
      "Epoch: 005/007 | Batch 25000/40000 | Cost: 0.6601\n",
      "Epoch: 005/007 | Batch 26000/40000 | Cost: 0.6355\n",
      "Epoch: 005/007 | Batch 27000/40000 | Cost: 0.5029\n",
      "Epoch: 005/007 | Batch 28000/40000 | Cost: 0.6278\n",
      "Epoch: 005/007 | Batch 29000/40000 | Cost: 0.5953\n",
      "Epoch: 005/007 | Batch 30000/40000 | Cost: 0.5635\n",
      "Epoch: 005/007 | Batch 31000/40000 | Cost: 0.6831\n",
      "Epoch: 005/007 | Batch 32000/40000 | Cost: 0.6149\n",
      "Epoch: 005/007 | Batch 33000/40000 | Cost: 0.7188\n",
      "Epoch: 005/007 | Batch 34000/40000 | Cost: 0.7066\n",
      "Epoch: 005/007 | Batch 35000/40000 | Cost: 0.4952\n",
      "Epoch: 005/007 | Batch 36000/40000 | Cost: 0.6341\n",
      "Epoch: 005/007 | Batch 37000/40000 | Cost: 0.5567\n",
      "Epoch: 005/007 | Batch 38000/40000 | Cost: 0.7025\n",
      "Epoch: 005/007 | Batch 39000/40000 | Cost: 0.7756\n",
      "training accuracy: 66.56%\n",
      "valid accuracy: 66.07%\n",
      "Time elapsed: 312.29 min\n",
      "Epoch: 006/007 | Batch 000/40000 | Cost: 0.5848\n",
      "Epoch: 006/007 | Batch 1000/40000 | Cost: 0.6301\n",
      "Epoch: 006/007 | Batch 2000/40000 | Cost: 0.8144\n",
      "Epoch: 006/007 | Batch 3000/40000 | Cost: 0.8441\n",
      "Epoch: 006/007 | Batch 4000/40000 | Cost: 0.6971\n",
      "Epoch: 006/007 | Batch 5000/40000 | Cost: 0.7311\n",
      "Epoch: 006/007 | Batch 6000/40000 | Cost: 0.4821\n",
      "Epoch: 006/007 | Batch 7000/40000 | Cost: 0.7788\n",
      "Epoch: 006/007 | Batch 8000/40000 | Cost: 0.6366\n",
      "Epoch: 006/007 | Batch 9000/40000 | Cost: 0.5183\n",
      "Epoch: 006/007 | Batch 10000/40000 | Cost: 0.8635\n",
      "Epoch: 006/007 | Batch 11000/40000 | Cost: 0.6176\n",
      "Epoch: 006/007 | Batch 12000/40000 | Cost: 0.5459\n",
      "Epoch: 006/007 | Batch 13000/40000 | Cost: 0.4964\n",
      "Epoch: 006/007 | Batch 14000/40000 | Cost: 0.6970\n",
      "Epoch: 006/007 | Batch 15000/40000 | Cost: 0.5554\n",
      "Epoch: 006/007 | Batch 16000/40000 | Cost: 0.4984\n",
      "Epoch: 006/007 | Batch 17000/40000 | Cost: 0.8846\n",
      "Epoch: 006/007 | Batch 18000/40000 | Cost: 0.6667\n",
      "Epoch: 006/007 | Batch 19000/40000 | Cost: 0.7259\n",
      "Epoch: 006/007 | Batch 20000/40000 | Cost: 0.5400\n",
      "Epoch: 006/007 | Batch 21000/40000 | Cost: 0.6425\n",
      "Epoch: 006/007 | Batch 22000/40000 | Cost: 0.6746\n",
      "Epoch: 006/007 | Batch 23000/40000 | Cost: 0.6304\n",
      "Epoch: 006/007 | Batch 24000/40000 | Cost: 0.5654\n",
      "Epoch: 006/007 | Batch 25000/40000 | Cost: 0.5185\n",
      "Epoch: 006/007 | Batch 26000/40000 | Cost: 0.7329\n",
      "Epoch: 006/007 | Batch 27000/40000 | Cost: 0.6759\n",
      "Epoch: 006/007 | Batch 28000/40000 | Cost: 0.7099\n",
      "Epoch: 006/007 | Batch 29000/40000 | Cost: 0.5036\n",
      "Epoch: 006/007 | Batch 30000/40000 | Cost: 0.5634\n",
      "Epoch: 006/007 | Batch 31000/40000 | Cost: 0.7095\n",
      "Epoch: 006/007 | Batch 32000/40000 | Cost: 0.6022\n",
      "Epoch: 006/007 | Batch 33000/40000 | Cost: 0.7446\n",
      "Epoch: 006/007 | Batch 34000/40000 | Cost: 0.4255\n",
      "Epoch: 006/007 | Batch 35000/40000 | Cost: 0.6788\n",
      "Epoch: 006/007 | Batch 36000/40000 | Cost: 0.5680\n",
      "Epoch: 006/007 | Batch 37000/40000 | Cost: 0.7014\n",
      "Epoch: 006/007 | Batch 38000/40000 | Cost: 0.4740\n",
      "Epoch: 006/007 | Batch 39000/40000 | Cost: 0.9243\n",
      "training accuracy: 66.86%\n",
      "valid accuracy: 66.62%\n",
      "Time elapsed: 373.07 min\n",
      "Epoch: 007/007 | Batch 000/40000 | Cost: 0.6535\n",
      "Epoch: 007/007 | Batch 1000/40000 | Cost: 0.6787\n",
      "Epoch: 007/007 | Batch 2000/40000 | Cost: 0.5736\n",
      "Epoch: 007/007 | Batch 3000/40000 | Cost: 0.7170\n",
      "Epoch: 007/007 | Batch 4000/40000 | Cost: 0.4919\n",
      "Epoch: 007/007 | Batch 5000/40000 | Cost: 0.6535\n",
      "Epoch: 007/007 | Batch 6000/40000 | Cost: 0.6289\n",
      "Epoch: 007/007 | Batch 7000/40000 | Cost: 0.6485\n",
      "Epoch: 007/007 | Batch 8000/40000 | Cost: 0.8316\n",
      "Epoch: 007/007 | Batch 9000/40000 | Cost: 0.4720\n",
      "Epoch: 007/007 | Batch 10000/40000 | Cost: 0.6633\n",
      "Epoch: 007/007 | Batch 11000/40000 | Cost: 0.6426\n",
      "Epoch: 007/007 | Batch 12000/40000 | Cost: 0.6454\n",
      "Epoch: 007/007 | Batch 13000/40000 | Cost: 0.5747\n",
      "Epoch: 007/007 | Batch 14000/40000 | Cost: 0.4505\n",
      "Epoch: 007/007 | Batch 15000/40000 | Cost: 0.5499\n",
      "Epoch: 007/007 | Batch 16000/40000 | Cost: 0.4271\n",
      "Epoch: 007/007 | Batch 17000/40000 | Cost: 0.4662\n",
      "Epoch: 007/007 | Batch 18000/40000 | Cost: 0.6304\n",
      "Epoch: 007/007 | Batch 19000/40000 | Cost: 0.7347\n",
      "Epoch: 007/007 | Batch 20000/40000 | Cost: 0.6085\n",
      "Epoch: 007/007 | Batch 21000/40000 | Cost: 0.6404\n",
      "Epoch: 007/007 | Batch 22000/40000 | Cost: 0.8301\n",
      "Epoch: 007/007 | Batch 23000/40000 | Cost: 0.6294\n",
      "Epoch: 007/007 | Batch 24000/40000 | Cost: 0.7369\n",
      "Epoch: 007/007 | Batch 25000/40000 | Cost: 0.7307\n",
      "Epoch: 007/007 | Batch 26000/40000 | Cost: 0.6431\n",
      "Epoch: 007/007 | Batch 27000/40000 | Cost: 0.6072\n",
      "Epoch: 007/007 | Batch 28000/40000 | Cost: 0.7908\n",
      "Epoch: 007/007 | Batch 29000/40000 | Cost: 0.5934\n",
      "Epoch: 007/007 | Batch 30000/40000 | Cost: 0.6750\n",
      "Epoch: 007/007 | Batch 31000/40000 | Cost: 0.6580\n",
      "Epoch: 007/007 | Batch 32000/40000 | Cost: 0.6051\n",
      "Epoch: 007/007 | Batch 33000/40000 | Cost: 0.8502\n",
      "Epoch: 007/007 | Batch 34000/40000 | Cost: 0.5938\n",
      "Epoch: 007/007 | Batch 35000/40000 | Cost: 0.7937\n",
      "Epoch: 007/007 | Batch 36000/40000 | Cost: 0.7465\n",
      "Epoch: 007/007 | Batch 37000/40000 | Cost: 0.5848\n",
      "Epoch: 007/007 | Batch 38000/40000 | Cost: 0.5637\n",
      "Epoch: 007/007 | Batch 39000/40000 | Cost: 0.5801\n",
      "training accuracy: 67.13%\n",
      "valid accuracy: 66.78%\n",
      "Time elapsed: 434.59 min\n",
      "Total Training Time: 434.59 min\n",
      "Test accuracy: 66.60%\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    for batch_idx, batch_data in enumerate(train_iterator):\n",
    "        \n",
    "        text, text_lengths = batch_data.tweet\n",
    "        \n",
    "        ### FORWARD AND BACK PROP\n",
    "        logits = model(text, text_lengths)\n",
    "        cost = F.binary_cross_entropy_with_logits(logits, batch_data.label)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        cost.backward()\n",
    "        \n",
    "        ### UPDATE MODEL PARAMETERS\n",
    "        optimizer.step()\n",
    "        \n",
    "        ### LOGGING\n",
    "        if not batch_idx % 1000:\n",
    "            print (f'Epoch: {epoch+1:03d}/{NUM_EPOCHS:03d} | '\n",
    "                   f'Batch {batch_idx:03d}/{len(train_iterator):03d} | '\n",
    "                   f'Cost: {cost:.4f}')\n",
    "\n",
    "    with torch.set_grad_enabled(False):\n",
    "        print(f'training accuracy: '\n",
    "              f'{compute_binary_accuracy(model, train_iterator, device):.2f}%'\n",
    "              f'\\nvalid accuracy: '\n",
    "              f'{compute_binary_accuracy(model, valid_iterator, device):.2f}%')\n",
    "        \n",
    "    print(f'Time elapsed: {(time.time() - start_time)/60:.2f} min')\n",
    "    \n",
    "print(f'Total Training Time: {(time.time() - start_time)/60:.2f} min')\n",
    "print(f'Test accuracy: {compute_binary_accuracy(model, test_iterator, device):.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "def predict_sentiment(model, sentence):\n",
    "    model.eval()\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    indexed = [TWEET.vocab.stoi[t] for t in tokenized]\n",
    "    length = [len(indexed)]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    length_tensor = torch.LongTensor(length)\n",
    "    prediction = torch.sigmoid(model(tensor, length_tensor))\n",
    "    return prediction.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability positive:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3635202646255493"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Probability positive:')\n",
    "1-predict_sentiment(model, \"Feel pretty upset today.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability negative:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6364797353744507"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Probability negative:')\n",
    "predict_sentiment(model, \"Feel pretty upset today.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability positive:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8992006033658981"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Probability positive:')\n",
    "1 - predict_sentiment(model, \"Had a really great day.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit",
   "language": "python",
   "name": "python37664bit864d233676b44db89082be2dfb69e388"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
